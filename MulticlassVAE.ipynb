{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "n_classes = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Computation Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lrelu(x, alpha=0.3):\n",
    "    return tf.maximum(x, tf.multiply(x, alpha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def scale_and_shift_flat(x, labels, name='s_and_s'):\n",
    "#     with tf.variable_scope(name, reuse=None):\n",
    "#         axis = [1,2]\n",
    "#         x_shape = x.get_shape().as_list()\n",
    "#         beta = tf.get_variable( 'beta', [n_classes])\n",
    "#         gamma = tf.get_variable( 'gamma', [n_classes])\n",
    "\n",
    "#         class_shift = tf.gather(beta, labels)\n",
    "#         class_shift = tf.expand_dims(class_shift, 1)\n",
    "\n",
    "#         class_scale = tf.gather(gamma, labels)\n",
    "#         class_scale = tf.expand_dims(class_scale, 1)\n",
    "\n",
    "#         output = x + class_shift\n",
    "#         output *= class_scale\n",
    "#         return output\n",
    "\n",
    "def scale_and_shift_flat(x, labels, name='s_and_s'):\n",
    "    with tf.variable_scope(name, reuse=None):\n",
    "        axis = [1,2]\n",
    "        x_shape = x.get_shape().as_list()\n",
    "        print('x shape: {}'.format(x.get_shape().as_list()))\n",
    "        \n",
    "        beta = tf.get_variable( 'beta', [n_classes])\n",
    "        gamma = tf.get_variable( 'gamma', [n_classes])\n",
    "\n",
    "        class_shift = tf.gather(beta, labels)\n",
    "        class_shift = tf.expand_dims(class_shift, 1)\n",
    "        print('class shift shape: {}'.format(class_shift.get_shape().as_list()))\n",
    "\n",
    "        class_scale = tf.gather(gamma, labels)\n",
    "        class_scale = tf.expand_dims(class_scale, 1)\n",
    "        print('class scale shape: {}'.format(class_scale.get_shape().as_list()))\n",
    "\n",
    "        variance_epsilon = 0.01\n",
    "        mean, variance = tf.nn.moments(x, axis, keep_dims=True)\n",
    "        print('mean shape: {}'.format(mean.get_shape().as_list()))\n",
    "        print('variance shape: {}'.format(variance.get_shape().as_list()))\n",
    "        output = tf.nn.batch_normalization(x=x, mean=mean,\n",
    "                                           variance=variance,\n",
    "                                           offset=class_shift, scale=class_scale,\n",
    "                                           variance_epsilon=variance_epsilon)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scale_and_shift(x, labels, name='s_and_s'):\n",
    "    with tf.variable_scope(name, reuse=None):\n",
    "        axis = [1]\n",
    "        x_shape = x.get_shape().as_list()\n",
    "        beta = tf.get_variable( 'beta', [n_classes])\n",
    "        gamma = tf.get_variable( 'gamma', [n_classes])\n",
    "        \n",
    "        class_shift = tf.gather(beta, labels)\n",
    "        class_shift = tf.expand_dims(tf.expand_dims(tf.expand_dims(class_shift, 1), 1), 1)\n",
    "        \n",
    "        class_scale = tf.gather(gamma, labels)\n",
    "        class_scale = tf.expand_dims(tf.expand_dims(tf.expand_dims(class_scale, 1), 1), 1)\n",
    "        \n",
    "        variance_epsilon = 0.01\n",
    "        mean, variance = tf.nn.moments(x, axis, keep_dims=True)\n",
    "        output = tf.nn.batch_normalization(x=x, mean=mean,\n",
    "                                           variance=variance,\n",
    "                                           offset=class_shift, scale=class_scale,\n",
    "                                           variance_epsilon=variance_epsilon)\n",
    "        return output\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encoder(X_in, labels, keep_prob):\n",
    "    activation = lrelu\n",
    "    with tf.variable_scope(\"encoder\", reuse=None):\n",
    "        X = tf.reshape(X_in, shape=[-1, 210, 160, 3])\n",
    "        \n",
    "        x = tf.layers.conv2d(X, filters=64, kernel_size=4, strides=2, padding='same', activation=activation)\n",
    "        x = scale_and_shift(x, labels, name='s_and_s/1')\n",
    "        x = tf.nn.dropout(x, keep_prob)\n",
    "        \n",
    "        x = tf.layers.conv2d(x, filters=64, kernel_size=4, strides=2, padding='same', activation=activation)\n",
    "        x = scale_and_shift(x, labels, name='s_and_s/2')\n",
    "        x = tf.nn.dropout(x, keep_prob)\n",
    "        \n",
    "        x = tf.layers.conv2d(x, filters=64, kernel_size=4, strides=1, padding='same', activation=activation)\n",
    "        x = scale_and_shift(x, labels, name='s_and_s/3')\n",
    "        x = tf.nn.dropout(x, keep_prob)\n",
    "        \n",
    "        x = tf.contrib.layers.flatten(x)\n",
    "        mn = tf.layers.dense(x, units=n_latent)\n",
    "        sd = 0.5 * tf.layers.dense(x, units=n_latent)            \n",
    "        epsilon = tf.random_normal(tf.stack([tf.shape(x)[0], n_latent])) \n",
    "        z  = mn + tf.multiply(epsilon, tf.exp(sd))\n",
    "        return z, mn, sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decoder(sampled_z, labels, keep_prob):\n",
    "    with tf.variable_scope(\"decoder\", reuse=None):\n",
    "        x = tf.layers.dense(sampled_z, units=inputs_decoder, activation=lrelu)\n",
    "        x = scale_and_shift_flat(x, labels, name='s_and_s/4')\n",
    "        \n",
    "        x = tf.layers.dense(x, units=(inputs_decoder * 2 + 1), activation=lrelu)\n",
    "        x = scale_and_shift_flat(x, labels, name='s_and_s/5')\n",
    "        x = tf.reshape(x, reshaped_dim)\n",
    "        \n",
    "        x = tf.layers.conv2d_transpose(x, filters=64, kernel_size=4, strides=2, padding='same', activation=tf.nn.relu)\n",
    "        x = scale_and_shift(x, labels, name='s_and_s/6')\n",
    "        x = tf.nn.dropout(x, keep_prob)\n",
    "        \n",
    "        x = tf.layers.conv2d_transpose(x, filters=64, kernel_size=4, strides=1, padding='same', activation=tf.nn.relu)\n",
    "        x = scale_and_shift(x, labels, name='s_and_s/7')\n",
    "        x = tf.nn.dropout(x, keep_prob)\n",
    "        \n",
    "        x = tf.layers.conv2d_transpose(x, filters=64, kernel_size=4, strides=1, padding='same', activation=tf.nn.relu)\n",
    "        x = scale_and_shift(x, labels, name='s_and_s/8')\n",
    "        \n",
    "        x = tf.contrib.layers.flatten(x)\n",
    "        x = tf.layers.dense(x, units=210*160*3, activation=tf.nn.sigmoid)\n",
    "        img = tf.reshape(x, shape=[-1, 210, 160, 3])\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: [None, 24]\n",
      "class shift shape: [None, 1]\n",
      "class scale shape: [None, 1]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid reduction dimension 2 for input with 2 dimensions. for 'decoder/s_and_s/4/moments/Mean' (op: 'Mean') with input shapes: [?,24], [2] and with computed input tensors: input[1] = <1 2>.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda2/envs/venv_3/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[1;32m    670\u001b[0m           \u001b[0mgraph_def_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_def_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shapes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m           input_tensors_as_shapes, status)\n\u001b[0m\u001b[1;32m    672\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/venv_3/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/venv_3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    467\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Invalid reduction dimension 2 for input with 2 dimensions. for 'decoder/s_and_s/4/moments/Mean' (op: 'Mean') with input shapes: [?,24], [2] and with computed input tensors: input[1] = <1 2>.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-9264273d7910>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0msampled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mdec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0munreshaped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m210\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m160\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-7054a6410745>\u001b[0m in \u001b[0;36mdecoder\u001b[0;34m(sampled_z, labels, keep_prob)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"decoder\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampled_z\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_decoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlrelu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscale_and_shift_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m's_and_s/4'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs_decoder\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlrelu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-748e810919a4>\u001b[0m in \u001b[0;36mscale_and_shift_flat\u001b[0;34m(x, labels, name)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mvariance_epsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_dims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mean shape: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'variance shape: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/venv_3/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py\u001b[0m in \u001b[0;36mmoments\u001b[0;34m(x, axes, shift, name, keep_dims)\u001b[0m\n\u001b[1;32m    636\u001b[0m       \u001b[0;31m# Compute true mean while keeping the dims for proper broadcasting.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m       shift = array_ops.stop_gradient(\n\u001b[0;32m--> 638\u001b[0;31m           math_ops.reduce_mean(y, axes, keep_dims=True))\n\u001b[0m\u001b[1;32m    639\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m       \u001b[0mshift\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshift\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/venv_3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mreduce_mean\u001b[0;34m(input_tensor, axis, keep_dims, name, reduction_indices)\u001b[0m\n\u001b[1;32m   1338\u001b[0m       \u001b[0m_ReductionDims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m       \u001b[0mkeep_dims\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1340\u001b[0;31m       name=name)\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/venv_3/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36m_mean\u001b[0;34m(input, reduction_indices, keep_dims, name)\u001b[0m\n\u001b[1;32m   1340\u001b[0m   result = _op_def_lib.apply_op(\"Mean\", input=input,\n\u001b[1;32m   1341\u001b[0m                                 \u001b[0mreduction_indices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreduction_indices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1342\u001b[0;31m                                 keep_dims=keep_dims, name=name)\n\u001b[0m\u001b[1;32m   1343\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/venv_3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36mapply_op\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    766\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    767\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 768\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    769\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/venv_3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[1;32m   2336\u001b[0m                     original_op=self._default_original_op, op_def=op_def)\n\u001b[1;32m   2337\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcompute_shapes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2338\u001b[0;31m       \u001b[0mset_shapes_for_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2339\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2340\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_record_op_seen_by_control_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/venv_3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mset_shapes_for_outputs\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   1717\u001b[0m       \u001b[0mshape_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1719\u001b[0;31m   \u001b[0mshapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1720\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshapes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1721\u001b[0m     raise RuntimeError(\n",
      "\u001b[0;32m~/anaconda2/envs/venv_3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcall_with_requiring\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   1667\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1668\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1669\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcall_cpp_shape_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequire_shape_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1671\u001b[0m   \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/venv_3/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36mcall_cpp_shape_fn\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[1;32m    608\u001b[0m     res = _call_cpp_shape_fn_impl(op, input_tensors_needed,\n\u001b[1;32m    609\u001b[0m                                   \u001b[0minput_tensors_as_shapes_needed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m                                   debug_python_shape_fn, require_shape_fn)\n\u001b[0m\u001b[1;32m    611\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m       \u001b[0;31m# Handles the case where _call_cpp_shape_fn_impl calls unknown_shape(op).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/venv_3/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[1;32m    674\u001b[0m       \u001b[0mmissing_shape_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmissing_shape_fn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid reduction dimension 2 for input with 2 dimensions. for 'decoder/s_and_s/4/moments/Mean' (op: 'Mean') with input shapes: [?,24], [2] and with computed input tensors: input[1] = <1 2>."
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "batch_size = 4\n",
    "n_classes = 10\n",
    "\n",
    "X_in = tf.placeholder(dtype=tf.float32, shape=[None, 210, 160, 3], name='X')\n",
    "Labels = tf.placeholder(dtype=tf.int32, shape=[None], name='Labels')\n",
    "Y = tf.placeholder(dtype=tf.float32, shape=[None, 210, 160, 3], name='Y')\n",
    "Y_flat = tf.reshape(Y, shape=[-1, 210 * 160 * 3])\n",
    "keep_prob = tf.placeholder(dtype=tf.float32, shape=(), name='keep_prob')\n",
    "\n",
    "dec_in_channels = 1\n",
    "n_latent = 8\n",
    "\n",
    "reshaped_dim = [-1, 7, 7, dec_in_channels]\n",
    "inputs_decoder = 49 * dec_in_channels // 2\n",
    "\n",
    "sampled, mn, sd = encoder(X_in, Labels, keep_prob)\n",
    "dec = decoder(sampled, Labels, keep_prob)\n",
    "\n",
    "unreshaped = tf.reshape(dec, [-1, 210*160*3])\n",
    "img_loss = tf.reduce_sum(tf.squared_difference(unreshaped, Y_flat), 1)\n",
    "latent_loss = -0.5 * tf.reduce_sum(1.0 + 2.0 * sd - tf.square(mn) - tf.exp(2.0 * sd), 1)\n",
    "loss = tf.reduce_mean(img_loss + latent_loss)\n",
    "\n",
    "# introduce variable learning rate\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "starter_learning_rate = 0.0005\n",
    "learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "                                           1000, 0.96, staircase=True)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gather New Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 500 files.\n"
     ]
    }
   ],
   "source": [
    "directories = ['jamesbond', 'spaceinvaders', 'tutankham', 'venture', 'zaxxon']\n",
    "state_label_pairs = []\n",
    "for i, root_dir in enumerate(directories):\n",
    "    for dir_name, subdir_list, file_list in os.walk(root_dir+'/'):\n",
    "        for fname in file_list:\n",
    "            state_label_pairs.append((root_dir + '/' + fname, i))\n",
    "\n",
    "print('Found {} files.'.format(len(state_label_pairs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_image(filename):\n",
    "    image = np.load(filename)\n",
    "    print(image.shape)\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(210, 160, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAANEAAAD8CAYAAADpCEEHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10cd47d68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(210, 160, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAANEAAAD8CAYAAADpCEEHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nySEyy+QQmWVyiMwy/T8WlRyhIM5d6QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11f3aef98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(210, 160, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAANEAAAD8CAYAAADpCEEHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11f41da20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(210, 160, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAANEAAAD8CAYAAADpCEEHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4xLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvAOZPmwAAEGtJREFUeJzt3XGMHOV9xvHvUxOQCihADRSwUx/I\noQUqOQQcJIrrJoEYC9VQNalNVawW1QRBBciV6piq4S/UpDGEKK0jR0GYKBhoUgKyqIPjhpo/CthO\njDExDgY78WHXDhABaSNSk1//mNmwt969m7t353Zm7vlIo919d3b3nfU9ft+Z3fmtIgIzm7jfGHQH\nzOrOITJL5BCZJXKIzBI5RGaJHCKzRKWFSNICSbsl7ZG0oqzXMRs0lfE5kaRpwI+Ay4FhYAuwJCJ+\n2PcXMxuwskaiucCeiHglIn4JPAgsKum1zAbqmJKe9yxgf9vtYeAjvVaW5K9NWBW9FhGnjrVSWSFS\nl7YRQZG0DFhW0uub9cOPi6xUVoiGgZltt2cAB9pXiIg1wBrwSGT1VtY+0RZgtqQhSccCi4HHSnot\ns4EqZSSKiCOSbga+A0wD7o2IF8p4LbNBK+UQ97g74emcVdO2iLhorJX8jQWzRA6RWSKHyCyRQ2SW\nyCEyS+QQmSVyiMwSOURmiRwis0QOkVmisr7FbTW2cePFI25ffvmWAfWkHjwS2QitAF1++ZZfh6cz\nVDaSQ2SWyCEyS+QQmSVyiMwS+aQ8O4qPzv1aoZPyiIgJLWSFSL4H7AJeAG7J2+8AXgW258vCAs8V\nXrxUcNlaJAspnxMdAZZHxPclnQhsk7Qxv+/uiPhCwnOb1caEQxQRB4GD+fW3Je0iK9poNqX05cCC\npFnAh4Bn8qabJe2QdK+kk/vxGmZVlRwiSScA3wJujYi3gNXAOcAcspFqVY/HLZO0VdLW1D6YDVLS\n0TlJ7wPWA9+JiLu63D8LWB8RF4zxPBPvhFl5yi2ZJUnA14Bd7QGSdEbbatcAOyf6GmZ1kHJ07lLg\nL4DnJW3P21YCSyTNITtEuA+4IamHZhXnD1vNemtWBdTXXntt0F0w66o2IZo+ffqgu2DWVW1CZFZV\nDpFZIofILJFDZJbIITJL5BCZJXKIzBI5RGaJHCKzRA6RWSKHyCyRQ2SWyCEyS+QQmSVyiMwSJf/I\nl6R9wNvAu8CRiLhI0inAQ8AsslPEPxURP0t9LbMq6tdI9EcRMaftVNoVwKaImA1sym+bNVJZ07lF\nwNr8+lrg6pJex2zg+hGiAJ6QtE3Ssrzt9LzMcKvc8Gl9eB2zSurHDx9fGhEHJJ0GbJT0YpEH5YFb\nNuaKZhWXPBJFxIH88jDwCDAXONQq4phfHu7yuDURcVGh338xq7CkEEk6Pv9ZFSQdD1xBVvH0MWBp\nvtpS4NGU1zGrstTp3OnAI1lFYY4BHoiIDZK2AA9Luh74CfDJxNcxqyxXQDXrrVkVUM2qqh9H56yG\nRivL7Gqz4zTRHz7u58Lgf+B2yi5PL1hQqG2KLoV++Nj7RFPYujPPHHF7yYEDrDvzTJYcODCgHlWO\n94mst84A9WqzsXmfaArzyNMfns5NQXfeObNr+8qV+ye5J5VXaDrnEJn15n0igyP7buXIvlsH3Y1G\n80jUAK2QHDPriwPuSeN4JDKbDB6JauDIvls5ZtYXPeJMPh9YqIv2cLQCY5XgEFVFrx17h6XyCoXI\nH7b2SeeUq53D0mwOUUGd4XBgrMXTuS5G+1zFIZlSyt0nknQuWZXTlrOBfwBOAv4a+GnevjIiHh/j\nuSoVIrPc5B1YkDQNeBX4CPCXwM8j4gvjeLxDZFU0qR+2fgx4OSJ+3KfnM6uNfoVoMbCu7fbNknZI\nulfSyX16DbNKSg6RpGOBPwb+NW9aDZwDzAEOAqt6PG6ZpK2Stqb2wWyQkveJJC0CboqIK7rcNwtY\nHxEXjPEc3ieyKpq0faIltE3lWuWDc9eQVUQ1a6ykD1sl/SZwOXBDW/PnJc0hq5ayr+M+s8bxh61m\nvfl8IrPJ4BCZJXKIzBI5RGaJHCKzRA6RWSKHyCyRQ2SWyCEyS+QQmSVyiMwSOURmiRwis0QOkVki\nh8gskUNklsghMktUKER56avDkna2tZ0iaaOkl/LLk/N2SfqSpD152awLy+q8WRUUHYnuAxZ0tK0A\nNkXEbGBTfhvgSmB2viwjK6Fl1liFQhQRm4E3OpoXAWvz62uBq9va74/M08BJHRWAzBolZZ/o9Ig4\nCJBfnpa3nwXsb1tvOG8bwcUbrSnK+H0idWk7qppPRKwB1oCr/Vi9pYxEh1rTtPzycN4+DMxsW28G\ncCDhdcwqLSVEjwFL8+tLgUfb2q/Lj9JdArzZmvZZMftXL2H/6iWD7oYVVGg6J2kdMB+YLmkY+Czw\nj8DDkq4HfgJ8Ml/9cWAhsAf4X7LfKzJrrEIhiohe/y1+rMu6AdyU0imzOvE3FswSuRa3WW+uxW02\nGRwis0QOkVkih8gskUNklsghMkvkEJklcojMEjlEZonKOJ9oymv/BvbMG9cNsCc2KSJi4AvZSXu1\nXvavXjLisrPdSy2XrUX+fj2dM0vkL6D20Wgn0nlaV0v+Aupk6wxK67YD1Gweicx6689I1KP66T9J\nejGvcPqIpJPy9lmSfiFpe758JW0bbDQLLsiWou1F77fxGXMkkjQP+DlZQcYL8rYrgP+IiCOSPgcQ\nEX8naRawvrVe4U54JErSGYgNO7u3d9qwc/T7rdhINObnRBGxOQ9He9sTbTefBv50vL0bj1tuuaXM\np6+93ZvuGXG79X51tnfy+zq6e+4Z/f1r6ceHrX8FPNR2e0jSD4C3gL+PiKe6PUjSMrJa3TZBvUIy\nVnisv5JCJOl24AjwjbzpIPCBiHhd0oeBb0s6PyLe6nysK6BaU0z4ELekpcBVwJ9H62sHEe9ExOv5\n9W3Ay8AH+9FRm7i/Ofd3B92FRit0iLvzgIGkBcBdwB9GxE/b1jsVeCMi3pV0NvAU8PsR0fmLEp3P\nP3onPj5mF6ekBf+d9vgNv92ffjTWd/t3iHsd8F/AuZKG84qnXwZOBDZ2HMqeB+yQ9BzwTeDTYwXI\nynHt3KFBd2HKqMeHrR6JuioyEl07d4gHnt3b9T6PRGMoOBLV4lSIOz86c+yVpqDND+wfc50Hnt3b\nM0h+X0e38rtjv7/g786ZJXOIGqjb/pD3kcpTi+nc5s3FhtUpZ0Z+OXz0Xa0pXCs83aZzfl/7oxYh\nsvF54Nm9LL/7YpYvmf5eY4+DC5bO07mGWX73xSNur7ptC6tu2zKg3kwNtRiJ5s3zUaTRdB6law9S\n6/qq27aw/O6LWXXbFuZd6/eziA0bfHRuSuo26ngkKpdD1ECdoemc4ll/1WI6t3//UV8CtzZDl70f\ngL1PvdlznfYg+f3sL49EU4indeVwiBrI07fJVYvp3N69vacpNn5+P/vLI9EU4alceRwis0QO0RTh\n/aTy1GKfaGjo/YPuQj10fFG7NYXrDJDfz6KK7TtOtALqHZJebat0urDtvs9I2iNpt6RPTKjvlqQ9\nPK2v+lh5ikzn7gMWdGm/OyLm5MvjAJLOAxYD5+eP+RdJ0/rVWRs/B6h8E6qAOopFwIMR8Q6wV9Ie\nYC5ZoRMbgPapXOubDdZfKftEN0u6DtgKLI+InwFnkZUVbhnO247iCqjlWnXbFjbsfK8e94adcOON\nDlEZJnp0bjVwDjCHrOrpqrxdXdbtWsknItZExEVFqqnY+LWK1W/YyYgwWf9NKEQRcSgi3o2IXwFf\nJZuyQTbytJ+sMgM4kNZFS+UAlWtCIZJ0RtvNa4DWkbvHgMWSjpM0BMwGnk3rok1U63eI/BMq5Rpz\nnyivgDofmC5pGPgsMF/SHLKp2j7gBoCIeEHSw8APyQrd3xQR75bTdRtNKzz+Qa/y1aIC6oJuB9ht\nbMMd+0MzRl3bOmzY4B8+NjwKTQaHqOG8P1Q+h6jhPBKVzyFqMI9Ck8MhajAf3p4cPhWiwfYOj/wq\nv9/H8erTqRBWb94nKp9DZJaoFtM5V6dJlH/I6vexHB6JzBI5RGaJHCKzRA6RWSKHyCxRLY7O2QT5\n1IdJ4ZHILNFEizc+1Fa4cZ+k7Xn7LEm/aLvvK2V23qwKikzn7gO+DNzfaoiIP2tdl7SKkV8yejki\n5vSrg2ZVl1S8UZKATwEf7W+3zOojdZ/oMuBQRLzU1jYk6QeS/lPSZYnPb1Z5qUfnlgDr2m4fBD4Q\nEa9L+jDwbUnnR8RRv7TrCqjWFBMeiSQdA/wJ8FCrLSLeiYjX8+vbgJeBD3Z7vCugWlOkTOc+DrwY\nEcOtBkmntn4FQtLZZMUbX0nrolm1FTnEvY7sVx3OlTQs6fr8rsWMnMoBzAN2SHoO+Cbw6Yh4o58d\nNquaWhRvNBsQF280mwwOkVkih8gskUNklqiWp0I8+eSTI27Pnz9/IP0wAyAiBr6Q/c7RqMv6lXNG\nXHa7z4uXPi9bi/z91uoQ9/qVo385/Ko7t/elP2a55h3ivurO7ew44fURS6vNAbJBqdVIZDbJmjcS\nmVWRQ2SWyCEyS+QQmSVyiMwSOURmiRwis0QOkVmiIqeHz5T0PUm7JL0g6Za8/RRJGyW9lF+enLdL\n0pck7ZG0Q9KFZW+E2SAVGYmOAMsj4veAS4CbJJ0HrAA2RcRsYFN+G+BKsgIls8lKYq3ue6/NKmTM\nEEXEwYj4fn79bWAXcBawCFibr7YWuDq/vgi4PzJPAydJOqPvPTeriHHtE+XlhD8EPAOcHhEHIQsa\ncFq+2lnA/raHDedtZo1U+KQ8SScA3wJujYi3sjLc3Vft0nbUF0xdAdWaotBIJOl9ZAH6RkT8W958\nqDVNyy8P5+3DwMy2h88ADnQ+pyugWlMUOTon4GvAroi4q+2ux4Cl+fWlwKNt7dflR+kuAd5sTfvM\nGqnAqdt/QDYd2wFsz5eFwG+RHZV7Kb88JV9fwD+T1eF+HrioH6eHe/EygKV5p4ebTTKflGc2GRwi\ns0QOkVkih8gskUNklqgqZYRfA/4nv2yK6TRne5q0LVB8e36nyJNV4hA3gKStTfr2QpO2p0nbAv3f\nHk/nzBI5RGaJqhSiNYPuQJ81aXuatC3Q5+2pzD6RWV1VaSQyq6WBh0jSAkm788ImK8Z+RPVI2ifp\neUnbJW3N27oWcqkiSfdKOixpZ1tbbQvR9NieOyS9mv8bbZe0sO2+z+Tbs1vSJ8b9ggP+hbxpZKdM\nnA0cCzwHnDfIPk1wO/YB0zvaPg+syK+vAD436H6O0v95wIXAzrH6T3YazL+TnfJyCfDMoPtfcHvu\nAP62y7rn5X93xwFD+d/jtPG83qBHornAnoh4JSJ+CTxIVuikCXoVcqmciNgMvNHRXNtCND22p5dF\nwIMR8U5E7AX2kP1dFjboEDWlqEkAT0jalteOgN6FXOqiiYVobs6noPe2Ta+Tt2fQISpU1KQGLo2I\nC8lq7t0kad6gO1Siuv6brQbOAeYAB4FVeXvy9gw6RIWKmlRdRBzILw8Dj5BNB3oVcqmLpEI0VRMR\nhyLi3Yj4FfBV3puyJW/PoEO0BZgtaUjSscBiskIntSHpeEkntq4DVwA76V3IpS4aVYimY7/tGrJ/\nI8i2Z7Gk4yQNkVXufXZcT16BIykLgR+RHRW5fdD9mUD/zyY7uvMc8EJrG+hRyKWKC7CObIrzf2T/\nM1/fq/9MoBBNRbbn63l/d+TBOaNt/dvz7dkNXDne1/M3FswSDXo6Z1Z7DpFZIofILJFDZJbIITJL\n5BCZJXKIzBI5RGaJ/h8MheyoIra4fAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11f482cc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_losses = []\n",
    "avg_img_losses = []\n",
    "sparse_batch_losses = []\n",
    "\n",
    "for i in range(1):\n",
    "    next_batch = random.sample(state_label_pairs, batch_size)\n",
    "    batch = [read_image(b[0]) for b in next_batch]\n",
    "    labels = [lab[1] for lab in next_batch]\n",
    "    batch_loss, batch_img_loss, _ = sess.run([loss, img_loss, optimizer], feed_dict = {X_in: batch, Y: batch, Labels: labels, keep_prob: 0.8})\n",
    "    batch_losses.append(batch_loss)\n",
    "    avg_img_losses.append(np.mean(batch_img_loss))\n",
    "        \n",
    "    if not i % 200:\n",
    "        batch_loss, decoded, batch_img_loss, mu, sigm = sess.run([loss, dec, img_loss, mn, sd], feed_dict = {X_in: batch, Y: batch, Labels: labels, keep_prob: 1.0})\n",
    "        batch_losses.append(batch_loss)\n",
    "        sparse_batch_losses.append(batch_loss)\n",
    "        avg_img_losses.append(np.mean(batch_img_loss))\n",
    "        \n",
    "        plt.imsave(fname='multi_vae_results/iteration_{}_original.png'.format(i), arr=np.reshape(batch[0], [210, 160, 3]), format='png')\n",
    "        plt.imsave(fname='multi_vae_results/iteration_{}_reconstructed.png'.format(i), decoded[0], format='png')\n",
    "        \n",
    "        plt.title('Batch losses')\n",
    "        plt.plot(np.arange(len(batch_losses)), batch_losses)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('multi_vae_results/iteration_{}_batch_losses', dpi=300)\n",
    "        \n",
    "        plt.title('Sparse Batch losses')\n",
    "        plt.plot(np.arange(len(sparse_batch_losses)), sparse_batch_losses)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('multi_vae_results/iteration_{}_sparse_batch_losses', dpi=300)\n",
    "        \n",
    "        print('iteration: {}; batch loss: {}, mean img loss: {}'.format(i, batch_loss, np.mean(batch_img_loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample New Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-10-8eb0b940e899>, line 23)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-10-8eb0b940e899>\"\u001b[0;36m, line \u001b[0;32m23\u001b[0m\n\u001b[0;31m    plt.imshow(img)\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "n_samples = 10\n",
    "randoms = [np.random.normal(0, 1, n_latent) for _ in range(n_samples)]\n",
    "classes = [np.random.choice(n_classes) for _ in range(n_samples)]\n",
    "imgs = sess.run(dec, feed_dict = {sampled: randoms, keep_prob: 1.0})\n",
    "imgs = [np.reshape(imgs[i], [210, 160, 3]) for i in range(len(imgs))]\n",
    "\n",
    "for img in imgs:\n",
    "    plt.figure(figsize=(1,1))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    \n",
    "n_samples = 10\n",
    "randoms = [np.random.normal(0, 1, n_latent) for _ in range(n_samples)]\n",
    "labels = [np.random.choice(n_classes) for _ in range(n_samples)]\n",
    "imgs = sess.run(dec, feed_dict = {sampled: randoms, Labels: labels, keep_prob: 1.0})\n",
    "imgs = [np.reshape(imgs[i], [210, 160, 3]) for i in range(len(imgs))]\n",
    "\n",
    "for i, img, c in zip(range(n_samples, imgs, classes)):\n",
    "    plt.imsave(fname='multi_vae_results/reconstruction_{}_class_{}'.format(i, c), arr=img, format='png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Venv Python 3",
   "language": "python",
   "name": "venv_3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
